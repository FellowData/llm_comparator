import streamlit as st
from ui.components.model_filters import render_model_filters
from ui.components.model_selector import render_model_selector
from llm_client import query_llm  # Import existant
from response_evaluator import ResponseEvaluator, display_comparative_dashboard  # Import existant
from services.history_service import get_history_service

import streamlit as st
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

def render_history_tab(evaluator=None):

    st.title("üìú Prompt History")

    # Chargement de l'historique via le service
    history_service = get_history_service()
    history_data, data_source = history_service.load_history_with_source()
    

    if history_data:
        # Convertir les donn√©es en DataFrame
        df = pd.DataFrame(history_data)
        
        # V√©rifier si les colonnes d'√©valuation existent (nouvelles donn√©es)
        has_evaluation_data = 'evaluation_score' in df.columns
        
        if 'response' not in df.columns:
            df['response'] = ''
        
        # Traitement des sources et r√©ponses principales
        df["sources"] = df["response"].apply(
            lambda r: r.split("=== SOURCES ===")[1].strip() 
            if isinstance(r, str) and "=== SOURCES ===" in r 
            else ""
        )
        df["main_response"] = df["response"].apply(
            lambda r: r.split("=== SOURCES ===")[0].strip() 
            if isinstance(r, str) and "=== SOURCES ===" in r 
            else str(r) if r else ""
        )

        # === ANALYTICS DASHBOARD ===
        if has_evaluation_data:
            st.markdown("## üìä Analytics Dashboard")
            
            # Statistiques globales
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                avg_score = df['evaluation_score'].mean()
                st.metric("Score Moyen Global", f"{avg_score:.1f}/10")
            
            with col2:
                best_model = df.loc[df['evaluation_score'].idxmax(), 'model_name'].split(':')[0]
                st.metric("Meilleur Mod√®le", best_model)
            
            with col3:
                total_evaluations = len(df[df['evaluation_score'].notna()])
                st.metric("√âvaluations", total_evaluations)
            
            with col4:
                avg_sources = df['sources_score'].mean() if 'sources_score' in df.columns else 0
                st.metric("Score Sources Moyen", f"{avg_sources:.1f}/10")
            
            # Graphiques d'analyse
            tab_analytics, tab_models, tab_trends = st.tabs(["üìä Vue d'ensemble", "ü§ñ Par Mod√®le", "üìà Tendances"])
            
            with tab_analytics:
                col_chart1, col_chart2 = st.columns(2)
                
                with col_chart1:
                    # Distribution des scores globaux
                    import plotly.express as px
                    fig_hist = px.histogram(
                        df, 
                        x='evaluation_score', 
                        title='Distribution des Scores Globaux',
                        nbins=20,
                        labels={'evaluation_score': 'Score Global', 'count': 'Nombre'}
                    )
                    st.plotly_chart(fig_hist, use_container_width=True)
                
                with col_chart2:
                    # Scores moyens par cat√©gorie
                    if all(col in df.columns for col in ['readability_score', 'structure_score', 'sources_score', 'completeness_score', 'relevance_score']):
                        categories_scores = {
                            'Lisibilit√©': df['readability_score'].mean(),
                            'Structure': df['structure_score'].mean(),
                            'Sources': df['sources_score'].mean(),
                            'Compl√©tude': df['completeness_score'].mean(),
                            'Pertinence': df['relevance_score'].mean()
                        }
                        
                        fig_bar = px.bar(
                            x=list(categories_scores.keys()),
                            y=list(categories_scores.values()),
                            title='Scores Moyens par Cat√©gorie',
                            labels={'x': 'Cat√©gorie', 'y': 'Score Moyen'}
                        )
                        st.plotly_chart(fig_bar, use_container_width=True)
            
            with tab_models:
                # Comparaison par mod√®le
                if len(df['model_name'].unique()) > 1:
                    model_stats = df.groupby('model_name').agg({
                        'evaluation_score': ['mean', 'count'],
                        'readability_score': 'mean',
                        'structure_score': 'mean',
                        'sources_score': 'mean',
                        'completeness_score': 'mean',
                        'relevance_score': 'mean'
                    }).round(1)
                    
                    model_stats.columns = ['Score Moyen', 'Nb Tests', 'Lisibilit√©', 'Structure', 'Sources', 'Compl√©tude', 'Pertinence']
                    model_stats['Mod√®le'] = [name.split(':')[0] for name in model_stats.index]
                    model_stats = model_stats.reset_index(drop=True)
                    
                    st.dataframe(model_stats, use_container_width=True)
                    
                    # Graphique radar par mod√®le (top 3)
                    top_models = model_stats.nlargest(3, 'Score Moyen')
                    
                    import plotly.graph_objects as go
                    fig_radar = go.Figure()
                    
                    categories = ['Lisibilit√©', 'Structure', 'Sources', 'Compl√©tude', 'Pertinence']
                    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
                    
                    for i, row in top_models.iterrows():
                        values = [row['Lisibilit√©'], row['Structure'], row['Sources'], row['Compl√©tude'], row['Pertinence']]
                        
                        fig_radar.add_trace(go.Scatterpolar(
                            r=values,
                            theta=categories,
                            fill='toself',
                            name=row['Mod√®le'],
                            line_color=colors[i % len(colors)]
                        ))
                    
                    fig_radar.update_layout(
                        polar=dict(radialaxis=dict(visible=True, range=[0, 10])),
                        showlegend=True,
                        title="Top 3 Mod√®les - Comparaison Radar"
                    )
                    
                    st.plotly_chart(fig_radar, use_container_width=True)
                else:
                    st.info("Donn√©es insuffisantes pour la comparaison entre mod√®les")
            
            with tab_trends:
                # √âvolution temporelle
                if 'timestamp' in df.columns and len(df) > 5:
                    df['date'] = pd.to_datetime(df['timestamp']).dt.date
                    daily_scores = df.groupby('date')['evaluation_score'].mean().reset_index()
                    
                    fig_trend = px.line(
                        daily_scores, 
                        x='date', 
                        y='evaluation_score',
                        title='√âvolution du Score Moyen dans le Temps'
                    )
                    st.plotly_chart(fig_trend, use_container_width=True)
                    
                    # Tendance par mod√®le
                    model_trends = df.groupby(['date', 'model_name'])['evaluation_score'].mean().reset_index()
                    model_trends['model_short'] = model_trends['model_name'].str.split(':').str[0]
                    
                    fig_model_trend = px.line(
                        model_trends,
                        x='date',
                        y='evaluation_score',
                        color='model_short',
                        title='√âvolution par Mod√®le'
                    )
                    st.plotly_chart(fig_model_trend, use_container_width=True)
                else:
                    st.info("Pas assez de donn√©es pour analyser les tendances temporelles")
        
        st.markdown("---")
        
        # === NOUVELLE SECTION : ANALYSE BATCH HISTORIQUE ===
        if has_evaluation_data and len(df) > 5:  # Au moins 6 entr√©es pour l'analyse
            st.markdown("---")
            st.markdown("## üî¨ Analyse Batch Historique")
            
            # Bouton pour lancer l'analyse avanc√©e
            if st.button("üöÄ G√©n√©rer Analyse Comparative Historique"):
                # V√©rifier si l'evaluator est disponible
                if evaluator is None:
                    # Cr√©er un evaluator temporaire si n√©cessaire
                    evaluator = ResponseEvaluator()
                with st.spinner("Analyse en cours des donn√©es historiques..."):
                    # Pr√©parer les donn√©es pour l'analyse batch
                    historical_data = []
                    for _, row in df.iterrows():
                        historical_data.append((
                            row['prompt'],
                            row['response'],
                            row['model_name']
                        ))
                    
                    # Grouper par mod√®le pour l'analyse
                    evaluations_by_model = {}
                    for model_name in df['model_name'].unique():
                        model_entries = df[df['model_name'] == model_name]
                        model_evaluations = []
                        
                        for _, entry in model_entries.iterrows():
                            # Recr√©er l'objet EvaluationResult depuis les donn√©es stock√©es
                            from response_evaluator import EvaluationResult
                            eval_result = EvaluationResult(
                                overall_score=entry.get('evaluation_score', 0),
                                readability_score=entry.get('readability_score', 0),
                                structure_score=entry.get('structure_score', 0),
                                sources_score=entry.get('sources_score', 0),
                                completeness_score=entry.get('completeness_score', 0),
                                relevance_score=entry.get('relevance_score', 0),
                                details={},
                                recommendations=[]
                            )
                            model_evaluations.append(eval_result)
                        
                        evaluations_by_model[model_name] = model_evaluations
                    
                    # G√©n√©rer le rapport historique
                    historical_report = evaluator.generate_comparative_report(evaluations_by_model)
                    
                    # Afficher le dashboard historique
                    st.markdown("### üìà Rapport Historique Complet")
                    display_comparative_dashboard(historical_report)
                    
                    # Insights sp√©cifiques √† l'historique
                    st.markdown("### üîç Insights Historiques Suppl√©mentaires")
                    
                    # √âvolution dans le temps
                    # Filtrer les donn√©es avec des scores valides
                    df_valid_scores = df[df['evaluation_score'].notna()]

                    if len(df_valid_scores) > 0:
                        daily_best = df_valid_scores.groupby('date').apply(
                            lambda x: x.loc[x['evaluation_score'].idxmax(), 'model_name'] if len(x) > 0 else None
                        ).dropna().value_counts()
                        
                        if len(daily_best) > 0:
                            st.write(f"üèÜ Mod√®le le plus souvent class√© premier : **{daily_best.index[0]}** ({daily_best.iloc[0]} jours)")
                        else:
                            st.write("üìä Donn√©es insuffisantes pour d√©terminer le mod√®le dominant")
                    else:
                        st.write("‚ö†Ô∏è Aucune donn√©e d'√©valuation valide trouv√©e")


                    #if 'timestamp' in df.columns:
                    #    df['date'] = pd.to_datetime(df['timestamp']).dt.date
                    #    
                        # √âvolution du meilleur mod√®le dans le temps
                    #    daily_best = df.groupby('date').apply(
                    #        lambda x: x.loc[x['evaluation_score'].idxmax(), 'model_name']
                    #    ).value_counts()
                    #    
                    #    if len(daily_best) > 0:
                    #        st.write(f"üèÜ Mod√®le le plus souvent class√© premier : **{daily_best.index[0]}** ({daily_best.iloc[0]} jours)")
                        
                        
                        # Tendance d'am√©lioration
                        df_sorted = df_valid_scores.sort_values('timestamp')
                        if len(df_sorted) >= 6:  # Au moins 6 entr√©es pour comparer
                            recent_avg = df_sorted.tail(min(10, len(df_sorted)//2))['evaluation_score'].mean()
                            older_avg = df_sorted.head(min(10, len(df_sorted)//2))['evaluation_score'].mean()
                        
                        # Tendance d'am√©lioration
                        #df_sorted = df.sort_values('timestamp')
                        #recent_avg = df_sorted.tail(10)['evaluation_score'].mean()
                        #older_avg = df_sorted.head(10)['evaluation_score'].mean()
                        
                        if len(df_sorted) >= 6 and not pd.isna(recent_avg) and not pd.isna(older_avg):
                            if recent_avg > older_avg:
                                improvement = ((recent_avg - older_avg) / older_avg) * 100
                                st.write(f"üìà Am√©lioration g√©n√©rale : +{improvement:.1f}% sur les r√©centes √©valuations")
                            elif recent_avg < older_avg:
                                decline = ((older_avg - recent_avg) / older_avg) * 100
                                st.write(f"üìâ D√©gradation observ√©e : -{decline:.1f}% sur les r√©centes √©valuations")
                            else:
                                st.write("‚öñÔ∏è Performance stable dans le temps")
                        else:
                            st.write("üìä Donn√©es insuffisantes pour analyser les tendances")
        
        # Filtres am√©lior√©s
        st.subheader("üîç Filters")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Filtre par mod√®le
            available_models = df["model_name"].unique().tolist() if "model_name" in df.columns else []
            model_filter = st.multiselect("Filter by model", options=available_models)
            if model_filter:
                df = df[df["model_name"].isin(model_filter)]
        
        with col2:
            # Recherche dans les prompts
            prompt_search = st.text_input("Search in prompts:")
            if prompt_search:
                df = df[df["prompt"].str.contains(prompt_search, case=False, na=False)]
            # Recherche dans les r√©ponses principales
            main_response_search = st.text_input("Search in main_response:")
            if main_response_search:
                df = df[df["main_response"].str.contains(main_response_search, case=False)]                

        with col3:
            # Filtre par score (si disponible)
            if has_evaluation_data:
                score_range = st.slider(
                    "Score Range",
                    min_value=0.0,
                    max_value=10.0,
                    value=(0.0, 10.0),
                    step=0.1
                )
                df = df[(df['evaluation_score'] >= score_range[0]) & (df['evaluation_score'] <= score_range[1])]

        # Affichage du tableau avec scores
        st.subheader("üìã History Table")
        
        if has_evaluation_data:
            display_columns = [
                "timestamp", "model_name", "prompt", "evaluation_score",
                "readability_score", "structure_score", "sources_score",
                "main_response", "sources"
            ]
        else:
            display_columns = ["timestamp", "model_name", "prompt", "main_response", "sources"]
        
        # V√©rifier quelles colonnes existent r√©ellement
        available_columns = [col for col in display_columns if col in df.columns]
        
        if available_columns:
            # Reformater l'affichage pour de meilleures colonnes
            df_display = df[available_columns].copy()
            
            if has_evaluation_data:
                # Renommer les colonnes pour l'affichage
                df_display.columns = [
                    "Date", "Mod√®le", "Prompt", "Score Global",
                    "Lisibilit√©", "Structure", "Sources Score",
                    "R√©ponse", "Sources"
                ]
                
                # Formater les scores
                score_cols = ["Score Global", "Lisibilit√©", "Structure", "Sources Score"]
                for col in score_cols:
                    if col in df_display.columns:
                        df_display[col] = df_display[col].round(1)
            
            st.dataframe(df_display, use_container_width=True, hide_index=True)
        else:
            st.warning("‚ö†Ô∏è Unable to display data: required columns not found")

        # Bouton de t√©l√©chargement Excel via le service
        if len(df) > 0:
            if st.button("üìÖ Generate Excel Export"):
                excel_data = history_service.export_to_excel(history_data)
                
                if excel_data:
                    st.download_button(
                        label="‚¨áÔ∏è Download Excel File",
                        data=excel_data,
                        file_name=f"prompt_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                else:
                    st.error("‚ùå √âchec de g√©n√©ration du fichier Excel")

    else:
        st.info("No prompt history available yet.")

    # === SECTION MAINTENANCE (OPTIONNELLE) ===
    st.markdown("---")
    st.markdown("## üîß Maintenance")
    
    col_maint1, col_maint2, col_maint3 = st.columns(3)
    
    with col_maint1:
        if st.button("üßπ Nettoyer Anciens (90j)"):
            removed = history_service.cleanup_old_entries(90)
            if removed > 0:
                st.success(f"‚úÖ {removed} entr√©e(s) supprim√©e(s)")
            else:
                st.info("‚ÑπÔ∏è Aucune entr√©e √† supprimer")
    
    with col_maint2:
        if st.button("‚úÖ Valider Int√©grit√©"):
            is_valid = history_service.validate_data_integrity()
            if is_valid:
                st.success("‚úÖ Donn√©es int√®gres")
            else:
                st.warning("‚ö†Ô∏è Probl√®mes d√©tect√©s")
    
    with col_maint3:
        file_size = history_service.get_file_size()
        st.metric("üìÅ Taille Fichier", file_size)