import streamlit as st
import json
import plotly.graph_objects as go
import pandas as pd
from pathlib import Path
from datetime import datetime
from ui.components.model_filters import render_model_filters, get_filter_stats
from ui.components.model_selector import render_model_selector, render_selection_summary
from llm_client import query_llm  # Import existant
from response_evaluator import ResponseEvaluator  # Import existant
from services.history_service import get_history_service
from services.evaluation_service import get_evaluation_service, EvaluationRequest

def render_prompt_tab(model_service, evaluator):
    """Rendu complet du Tab 1 - Prompt & Compare"""
    st.title("ğŸ§  LLM Comparator")
    prompt = st.text_area("ğŸ’¬ Your prompt:", height=150)
    
    # Utilisation des composants extraits
    with st.expander("ğŸ¯ Select Models", expanded=True):
        filtered_models = render_model_filters(model_service)
        selected = render_model_selector(filtered_models, model_service)

    render_selection_summary(selected, model_service)

    if st.button("ğŸš€ Run", disabled=not (prompt and selected)) and prompt and selected:
        # Initialiser le service d'Ã©valuation et crÃ©er une session
        evaluation_service = get_evaluation_service()
        session_id = evaluation_service.create_evaluation_session()
        
        # Stocker les rÃ©ponses pour l'Ã©valuation comparative
        responses_for_evaluation = {}
        models_dict = model_service.get_raw_models_dict()  # Pour compatibilitÃ©
        for name in selected:
            model = models_dict[name]
            model_id = model["id"]
            provider = model["provider"]

            # Affichage enrichi avec indicateurs
            indicators = []
            if model.get("free", False):
                indicators.append("ğŸ†“")
            else:
                indicators.append("ğŸ’°")
            
            if model.get("web_search", False):
                indicators.append("ğŸŒ")
            
            header = f"### ğŸ¤– {' '.join(indicators)} {name}"
            st.markdown(header)
            
            with st.spinner("Generating response..."):
                instruction = (
                    "\n\nÃ€ la fin de ta rÃ©ponse, ajoute une section intitulÃ©e '=== SOURCES ===' "
                    "avec la liste des urls des sites web utilisÃ©es pour la rÃ©ponse."
                )
                full_prompt = f"{prompt.strip()}{instruction}"

                try:
                    content = query_llm(provider, model_id, full_prompt)

                    if "=== SOURCES ===" in content:
                        answer_part, sources_part = content.split("=== SOURCES ===", 1)
                    else:
                        answer_part = content
                        sources_part = "*Aucune source identifiable fournie.*"

                    st.success("Response received.")
                    st.markdown("**ğŸ“ Answer:**")
                    st.write(answer_part.strip())
                    st.markdown("**ğŸ”— Sources:**")
                    st.info(sources_part.strip())
                    
                    # === Ã‰VALUATION AUTOMATIQUE ===
                    st.markdown("---")
                    with st.spinner("Ã‰valuation automatique en cours..."):
                        # CrÃ©er la demande d'Ã©valuation
                        eval_request = EvaluationRequest(
                            prompt=prompt,
                            response=content,
                            model_name=name
                        )
                        
                        # Ajouter Ã  la session (affichage automatique)
                        evaluation_service.add_to_session(session_id, eval_request, display_individual=True)
                        
                        # RÃ©cupÃ©rer l'Ã©valuation pour compatibilitÃ© avec le code existant
                        session = evaluation_service.get_session(session_id)
                        evaluation = session.evaluations[name]
                    
                    # Stocker pour comparaison AVANCÃ‰E
                    #responses_for_evaluation.append((prompt, content, name))
                    responses_for_evaluation[name] = {
                        'content': content,
                        'evaluation': evaluation
                    }
                    
                    # Save history avec Ã©valuation via le service
                    history_service = get_history_service()
                    save_success = history_service.save_evaluation_entry(
                        prompt, name, model_id, content, evaluation
                    )                    
                    if not save_success:
                        st.warning("âš ï¸ ProblÃ¨me lors de la sauvegarde de l'historique")                    
                    

                except Exception as e:
                    st.error(f"Error: {e}")
        
        # === NOUVELLE SECTION : RAPPORT COMPARATIF AVANCÃ‰ ===
        if len(responses_for_evaluation) > 1:
            st.markdown("---")
            st.markdown("## ğŸ† Analyse Comparative AvancÃ©e")
            
            # Tableau comparatif
            comparison_data = []
            for model_name, data in responses_for_evaluation.items():
                eval_result = data['evaluation']
                comparison_data.append({
                    'ModÃ¨le': model_name.split(':')[0].strip(),
                    'Score Global': eval_result.overall_score,
                    'ğŸ“– LisibilitÃ©': eval_result.readability_score,
                    'ğŸ—ï¸ Structure': eval_result.structure_score,
                    'ğŸ”— Sources': eval_result.sources_score,
                    'ğŸ“‹ ComplÃ©tude': eval_result.completeness_score,
                    'ğŸ¯ Pertinence': eval_result.relevance_score,
                    'ğŸ“Š Mots': eval_result.details['word_count']
                })
            
            df_comparison = pd.DataFrame(comparison_data)
            df_comparison = df_comparison.sort_values('Score Global', ascending=False)
            
            st.dataframe(df_comparison, use_container_width=True)
            
            # Graphique radar comparatif
            if len(comparison_data) <= 4:  # Ã‰viter la surcharge visuelle
                
                categories = ['LisibilitÃ©', 'Structure', 'Sources', 'ComplÃ©tude', 'Pertinence']
                
                fig = go.Figure()
                
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
                
                for i, (model_name, data) in enumerate(responses_for_evaluation.items()):
                    eval_result = data['evaluation']
                    values = [
                        eval_result.readability_score,
                        eval_result.structure_score,
                        eval_result.sources_score,
                        eval_result.completeness_score,
                        eval_result.relevance_score
                    ]
                    
                    fig.add_trace(go.Scatterpolar(
                        r=values,
                        theta=categories,
                        fill='toself',
                        name=model_name.split(':')[0].strip(),
                        line_color=colors[i % len(colors)]
                    ))
                
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 10]
                        )),
                    showlegend=True,
                    title="Comparaison Radar des Performances",
                    height=500
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            # Recommandations du meilleur modÃ¨le
            best_model = max(responses_for_evaluation.items(), 
                            key=lambda x: x[1]['evaluation'].overall_score)
            
            st.markdown(f"### ğŸ† Meilleur ModÃ¨le: {best_model[0].split(':')[0].strip()}")
            st.markdown(f"**Score: {best_model[1]['evaluation'].overall_score}/10**")
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**ğŸ¯ Points forts:**")
                eval_details = best_model[1]['evaluation']
                strengths = []
                if eval_details.readability_score >= 8:
                    strengths.append("ğŸ“– Excellente lisibilitÃ©")
                if eval_details.structure_score >= 8:
                    strengths.append("ğŸ—ï¸ TrÃ¨s bien structurÃ©")
                if eval_details.sources_score >= 8:
                    strengths.append("ğŸ”— Sources de qualitÃ©")
                if eval_details.completeness_score >= 8:
                    strengths.append("ğŸ“‹ RÃ©ponse complÃ¨te")
                if eval_details.relevance_score >= 8:
                    strengths.append("ğŸ¯ TrÃ¨s pertinent")
                
                for strength in strengths:
                    st.write(f"â€¢ {strength}")
            
            with col2:
                st.markdown("**âš ï¸ Points d'amÃ©lioration:**")
                weaknesses = []
                if eval_details.readability_score < 6:
                    weaknesses.append("ğŸ“– LisibilitÃ© Ã  amÃ©liorer")
                if eval_details.structure_score < 6:
                    weaknesses.append("ğŸ—ï¸ Structure Ã  revoir")
                if eval_details.sources_score < 6:
                    weaknesses.append("ğŸ”— Manque de sources")
                if eval_details.completeness_score < 6:
                    weaknesses.append("ğŸ“‹ RÃ©ponse incomplÃ¨te")
                if eval_details.relevance_score < 6:
                    weaknesses.append("ğŸ¯ Pertinence Ã  amÃ©liorer")
                
                if not weaknesses:
                    st.write("âœ… Aucun point faible majeur identifiÃ©")
                else:
                    for weakness in weaknesses:
                        st.write(f"â€¢ {weakness}")





            # GÃ©nÃ©ration du rapport comparatif
            with st.spinner("GÃ©nÃ©ration du rapport comparatif..."):
                # PrÃ©parer les donnÃ©es pour le rapport
                evaluations_by_model = {}
                for model_name, data in responses_for_evaluation.items():
                    evaluations_by_model[model_name] = [data['evaluation']]
                
                # GÃ©nÃ©rer le rapport
                comparative_report = evaluator.generate_comparative_report(evaluations_by_model)
                
                # Afficher le dashboard
                display_comparative_dashboard(comparative_report)
                
                # Option de sauvegarde du rapport
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ğŸ’¾ Sauvegarder Rapport JSON", disabled=True):
                        report_filename = f"data/comparative_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                        try:
                            save_evaluation_report(comparative_report, report_filename)
                            st.success(f"Rapport sauvegardÃ© : {report_filename}")
                        except Exception as e:
                            st.error(f"Erreur de sauvegarde : {e}")
                
                with col2:
                    # Export des donnÃ©es brutes du rapport
                    if st.button("ğŸ“Š Export DonnÃ©es Brutes", disabled=True):
                        report_json = json.dumps(comparative_report, indent=2, ensure_ascii=False)
                        st.download_button(
                            label="â¬‡ï¸ TÃ©lÃ©charger Rapport JSON",
                            data=report_json,
                            file_name=f"comparative_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )

