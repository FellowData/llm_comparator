"""
Service d'√©valuation automatique des r√©ponses LLM
√âvalue la qualit√© des r√©ponses selon plusieurs crit√®res objectifs
Version compl√®te avec fonctionnalit√©s avanc√©es
"""

import re
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from urllib.parse import urlparse
import streamlit as st
import json

# Remplacement de textstat par une impl√©mentation simple
def simple_flesch_reading_ease(text: str) -> float:
    """Calcul simplifi√© du score de lisibilit√© Flesch"""
    if not text.strip():
        return 0
    
    # Compter les phrases
    sentences = len(re.findall(r'[.!?]+', text))
    if sentences == 0:
        sentences = 1
    
    # Compter les mots
    words = len(text.split())
    if words == 0:
        return 0
    
    # Compter les syllabes (approximation)
    syllables = 0
    for word in text.split():
        syllables += count_syllables(word)
    
    # Formule Flesch
    if syllables == 0:
        syllables = words  # Fallback
    
    score = 206.835 - (1.015 * words / sentences) - (84.6 * syllables / words)
    return max(0, min(100, score))

def count_syllables(word: str) -> int:
    """Compte approximatif des syllabes dans un mot"""
    word = word.lower().strip('.,!?;:"()[]{}')
    if not word:
        return 1
    
    vowels = 'aeiouAEIOU√Ä√Å√Ç√É√Ñ√Ö√†√°√¢√£√§√•√à√â√ä√ã√®√©√™√´√å√ç√é√è√¨√≠√Æ√Ø√í√ì√î√ï√ñ√≤√≥√¥√µ√∂√ô√ö√õ√ú√π√∫√ª√º√ø'
    syllables = 0
    previous_was_vowel = False
    
    for char in word:
        is_vowel = char in vowels
        if is_vowel and not previous_was_vowel:
            syllables += 1
        previous_was_vowel = is_vowel
    
    # Les mots se terminant par 'e' muet
    if word.endswith('e') and syllables > 1:
        syllables -= 1
    
    return max(1, syllables)

def simple_flesch_kincaid_grade(text: str) -> float:
    """Calcul simplifi√© du grade Flesch-Kincaid"""
    if not text.strip():
        return 0
    
    sentences = len(re.findall(r'[.!?]+', text))
    if sentences == 0:
        sentences = 1
    
    words = len(text.split())
    if words == 0:
        return 0
    
    syllables = sum(count_syllables(word) for word in text.split())
    
    grade = (0.39 * words / sentences) + (11.8 * syllables / words) - 15.59
    return max(0, grade)

@dataclass
class EvaluationResult:
    """R√©sultat d'√©valuation d'une r√©ponse"""
    overall_score: float
    readability_score: float
    structure_score: float
    sources_score: float
    completeness_score: float
    relevance_score: float
    details: Dict[str, any]
    recommendations: List[str]

class ResponseEvaluator:
    """√âvaluateur automatique de r√©ponses LLM avec fonctionnalit√©s avanc√©es"""
    
    def __init__(self):
        self.weights = {
            'readability': 0.2,
            'structure': 0.2,
            'sources': 0.25,
            'completeness': 0.15,
            'relevance': 0.2
        }
    
    def evaluate_response(self, prompt: str, response: str, model_name: str = "") -> EvaluationResult:
        """√âvalue une r√©ponse selon tous les crit√®res"""
        
        # S√©parer la r√©ponse principale des sources
        main_response, sources_text = self._extract_main_and_sources(response)
        
        # Calcul des scores individuels
        readability = self._evaluate_readability(main_response)
        structure = self._evaluate_structure(main_response)
        sources = self._evaluate_sources(sources_text)
        completeness = self._evaluate_completeness(prompt, main_response)
        relevance = self._evaluate_relevance(prompt, main_response)
        
        # Score global pond√©r√©
        overall_score = (
            readability['score'] * self.weights['readability'] +
            structure['score'] * self.weights['structure'] +
            sources['score'] * self.weights['sources'] +
            completeness['score'] * self.weights['completeness'] +
            relevance['score'] * self.weights['relevance']
        )
        
        # G√©n√©ration des recommandations
        recommendations = self._generate_recommendations(
            readability, structure, sources, completeness, relevance
        )
        
        return EvaluationResult(
            overall_score=round(overall_score, 1),
            readability_score=round(readability['score'], 1),
            structure_score=round(structure['score'], 1),
            sources_score=round(sources['score'], 1),
            completeness_score=round(completeness['score'], 1),
            relevance_score=round(relevance['score'], 1),
            details={
                'readability': readability,
                'structure': structure,
                'sources': sources,
                'completeness': completeness,
                'relevance': relevance,
                'word_count': len(main_response.split()),
                'char_count': len(main_response),
                'model_name': model_name
            },
            recommendations=recommendations
        )
    
    def evaluate_batch_responses(self, prompts_responses: List[Tuple[str, str, str]]) -> Dict[str, List[EvaluationResult]]:
        """√âvalue plusieurs r√©ponses en lot pour comparaison"""
        results = {}
        
        for prompt, response, model_name in prompts_responses:
            if model_name not in results:
                results[model_name] = []
            
            evaluation = self.evaluate_response(prompt, response, model_name)
            results[model_name].append(evaluation)
        
        return results

    def generate_comparative_report(self, evaluations: Dict[str, List[EvaluationResult]]) -> Dict[str, any]:
        """G√©n√®re un rapport comparatif d√©taill√©"""
        report = {
            'summary': {},
            'rankings': {},
            'insights': [],
            'recommendations': []
        }
        
        # Calcul des moyennes par mod√®le
        for model_name, evals in evaluations.items():
            if not evals:
                continue
                
            avg_scores = {
                'overall': sum(e.overall_score for e in evals) / len(evals),
                'readability': sum(e.readability_score for e in evals) / len(evals),
                'structure': sum(e.structure_score for e in evals) / len(evals),
                'sources': sum(e.sources_score for e in evals) / len(evals),
                'completeness': sum(e.completeness_score for e in evals) / len(evals),
                'relevance': sum(e.relevance_score for e in evals) / len(evals)
            }
            
            report['summary'][model_name] = {
                'averages': avg_scores,
                'count': len(evals),
                'consistency': self._calculate_consistency(evals)
            }
        
        # Classement par crit√®re
        for criterion in ['overall', 'readability', 'structure', 'sources', 'completeness', 'relevance']:
            ranked = sorted(
                report['summary'].items(),
                key=lambda x: x[1]['averages'][criterion],
                reverse=True
            )
            report['rankings'][criterion] = [model for model, _ in ranked]
        
        # G√©n√©ration d'insights
        report['insights'] = self._generate_insights(report['summary'])
        report['recommendations'] = self._generate_comparative_recommendations(report['summary'])
        
        return report

    def _calculate_consistency(self, evaluations: List[EvaluationResult]) -> float:
        """Calcule la consistance des scores (inverse de l'√©cart-type)"""
        if len(evaluations) < 2:
            return 1.0
        
        scores = [e.overall_score for e in evaluations]
        mean_score = sum(scores) / len(scores)
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        std_dev = variance ** 0.5
        
        # Normaliser la consistance sur 0-1 (1 = tr√®s consistant)
        return max(0, 1 - (std_dev / 5))

    def _generate_insights(self, summary: Dict[str, Dict]) -> List[str]:
        """G√©n√®re des insights automatiques"""
        insights = []
        
        if not summary:
            return insights
        
        # Trouver le meilleur mod√®le global
        best_model = max(summary.items(), key=lambda x: x[1]['averages']['overall'])
        insights.append(f"üèÜ Meilleur mod√®le global: {best_model[0]} ({best_model[1]['averages']['overall']:.1f}/10)")
        
        # Identifier les sp√©cialistes
        specialists = {}
        for criterion in ['readability', 'structure', 'sources', 'completeness', 'relevance']:
            best_for_criterion = max(summary.items(), key=lambda x: x[1]['averages'][criterion])
            specialists[criterion] = best_for_criterion[0]
        
        # Mod√®les sp√©cialis√©s
        for criterion, model in specialists.items():
            if model != best_model[0]:
                score = summary[model]['averages'][criterion]
                insights.append(f"üéØ Sp√©cialiste {criterion}: {model} ({score:.1f}/10)")
        
        # Consistance
        most_consistent = max(summary.items(), key=lambda x: x[1]['consistency'])
        if most_consistent[1]['consistency'] > 0.8:
            insights.append(f"‚öñÔ∏è Plus consistant: {most_consistent[0]} (consistance: {most_consistent[1]['consistency']:.2f})")
        
        return insights

    def _generate_comparative_recommendations(self, summary: Dict[str, Dict]) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur la comparaison"""
        recommendations = []
        
        if len(summary) < 2:
            return recommendations
        
        # Recommandations par usage
        best_overall = max(summary.items(), key=lambda x: x[1]['averages']['overall'])
        best_sources = max(summary.items(), key=lambda x: x[1]['averages']['sources'])
        best_readability = max(summary.items(), key=lambda x: x[1]['averages']['readability'])
        
        recommendations.append(f"üìã Usage g√©n√©ral: Privil√©gier {best_overall[0]}")
        
        if best_sources[0] != best_overall[0]:
            recommendations.append(f"üîó Recherche document√©e: Utiliser {best_sources[0]}")
        
        if best_readability[0] != best_overall[0]:
            recommendations.append(f"üìñ Communication grand public: Choisir {best_readability[0]}")
        
        # D√©tection des mod√®les √† √©viter
        worst_model = min(summary.items(), key=lambda x: x[1]['averages']['overall'])
        if worst_model[1]['averages']['overall'] < 6:
            recommendations.append(f"‚ö†Ô∏è √Ä √©viter pour des t√¢ches critiques: {worst_model[0]}")
        
        return recommendations
    
    def _extract_main_and_sources(self, response: str) -> Tuple[str, str]:
        """S√©pare la r√©ponse principale des sources"""
        if "=== SOURCES ===" in response:
            parts = response.split("=== SOURCES ===", 1)
            return parts[0].strip(), parts[1].strip()
        return response.strip(), ""
    
    def _evaluate_readability(self, text: str) -> Dict[str, any]:
        """√âvalue la lisibilit√© du texte"""
        if not text.strip():
            return {'score': 0, 'level': 'Aucun texte', 'details': {}}
        
        try:
            # Score Flesch (0-100, plus haut = plus lisible)
            flesch_score = simple_flesch_reading_ease(text)
            # Grade Flesch-Kincaid (niveau scolaire)
            fk_grade = simple_flesch_kincaid_grade(text)
            
            # Normalisation du score Flesch sur 10
            if flesch_score >= 90:
                normalized_score = 10
                level = "Tr√®s facile"
            elif flesch_score >= 80:
                normalized_score = 8.5
                level = "Facile"
            elif flesch_score >= 70:
                normalized_score = 7.5
                level = "Assez facile"
            elif flesch_score >= 60:
                normalized_score = 6.5
                level = "Standard"
            elif flesch_score >= 50:
                normalized_score = 5.5
                level = "Assez difficile"
            elif flesch_score >= 30:
                normalized_score = 4
                level = "Difficile"
            else:
                normalized_score = 2
                level = "Tr√®s difficile"
            
            return {
                'score': normalized_score,
                'level': level,
                'details': {
                    'flesch_score': round(flesch_score, 1),
                    'grade_level': round(fk_grade, 1),
                    'avg_sentence_length': self._avg_sentence_length(text),
                    'avg_word_length': self._avg_word_length(text)
                }
            }
        except Exception as e:
            # Fallback simple si le calcul √©choue
            return self._simple_readability(text)
    
    def _simple_readability(self, text: str) -> Dict[str, any]:
        """Calcul de lisibilit√© simplifi√© en cas d'√©chec"""
        sentences = len(re.findall(r'[.!?]+', text))
        words = len(text.split())
        avg_sentence_length = words / max(sentences, 1)
        avg_word_length = sum(len(word) for word in text.split()) / max(words, 1)
        
        # Score simple bas√© sur les moyennes
        if avg_sentence_length < 15 and avg_word_length < 5:
            score = 8
            level = "Facile"
        elif avg_sentence_length < 20 and avg_word_length < 6:
            score = 6
            level = "Standard"
        else:
            score = 4
            level = "Difficile"
        
        return {
            'score': score,
            'level': level,
            'details': {
                'avg_sentence_length': round(avg_sentence_length, 1),
                'avg_word_length': round(avg_word_length, 1)
            }
        }
    
    def _evaluate_structure(self, text: str) -> Dict[str, any]:
        """√âvalue la structure et l'organisation du texte"""
        if not text.strip():
            return {'score': 0, 'issues': ['Aucun texte']}
        
        score = 5  # Score de base
        issues = []
        bonuses = []
        
        # Pr√©sence de paragraphes
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1:
            score += 1
            bonuses.append("Texte structur√© en paragraphes")
        else:
            issues.append("Manque de paragraphes")
        
        # Pr√©sence de listes ou √©num√©rations
        if re.search(r'^\s*[-‚Ä¢*]\s', text, re.MULTILINE) or re.search(r'^\s*\d+\.\s', text, re.MULTILINE):
            score += 1
            bonuses.append("Utilise des listes pour clarifier")
        
        # Pr√©sence de titres ou sous-sections
        if re.search(r'^#{1,6}\s', text, re.MULTILINE) or re.search(r'\*\*[^*]+\*\*', text):
            score += 1
            bonuses.append("Contient des titres/sous-sections")
        
        # Longueur appropri√©e
        word_count = len(text.split())
        if 50 <= word_count <= 500:
            score += 1
            bonuses.append("Longueur appropri√©e")
        elif word_count < 20:
            score -= 1
            issues.append("R√©ponse trop courte")
        elif word_count > 800:
            score -= 0.5
            issues.append("R√©ponse peut-√™tre trop longue")
        
        # Pr√©sence d'introduction/conclusion
        if text.lower().startswith(('en ', 'pour ', 'dans ', 'il ', 'la ', 'le ', 'cette ', 'ce ')):
            score += 0.5
            bonuses.append("Introduction claire")
        
        if any(word in text.lower()[-100:] for word in ['conclusion', 'r√©sum√©', 'finalement', 'en r√©sum√©', 'pour conclure']):
            score += 0.5
            bonuses.append("Conclusion pr√©sente")
        
        return {
            'score': min(10, max(0, score)),
            'issues': issues,
            'bonuses': bonuses,
            'details': {
                'paragraph_count': len(paragraphs),
                'word_count': word_count,
                'has_lists': bool(re.search(r'^\s*[-‚Ä¢*]\s', text, re.MULTILINE)),
                'has_headers': bool(re.search(r'^#{1,6}\s', text, re.MULTILINE))
            }
        }
    
    def _evaluate_sources(self, sources_text: str) -> Dict[str, any]:
        """√âvalue la qualit√© et pertinence des sources"""
        if not sources_text or sources_text == "*Aucune source identifiable fournie.*":
            return {
                'score': 0,
                'url_count': 0,
                'valid_urls': 0,
                'domain_diversity': 0,
                'issues': ['Aucune source fournie']
            }
        
        # Extraction des URLs
        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sources_text)
        
        if not urls:
            return {
                'score': 1,
                'url_count': 0,
                'valid_urls': 0,
                'domain_diversity': 0,
                'issues': ['Sources mentionn√©es mais pas d\'URLs valides']
            }
        
        # Validation des URLs
        valid_urls = []
        domains = set()
        reliable_domains = {'wikipedia.org', 'gov', 'edu', 'nature.com', 'sciencedirect.com', 'pubmed.ncbi.nlm.nih.gov'}
        
        for url in urls:
            try:
                parsed = urlparse(url)
                if parsed.netloc:
                    valid_urls.append(url)
                    domains.add(parsed.netloc.lower())
            except:
                pass
        
        # Calcul du score
        score = 0
        issues = []
        bonuses = []
        
        # Nombre de sources
        if len(valid_urls) >= 3:
            score += 3
            bonuses.append(f"{len(valid_urls)} sources fournies")
        elif len(valid_urls) >= 1:
            score += len(valid_urls)
            bonuses.append(f"{len(valid_urls)} source(s) fournie(s)")
        
        # Diversit√© des domaines
        domain_diversity = len(domains) / max(len(valid_urls), 1)
        if domain_diversity > 0.7:
            score += 2
            bonuses.append("Bonne diversit√© des sources")
        elif domain_diversity > 0.5:
            score += 1
        else:
            issues.append("Sources trop concentr√©es sur peu de domaines")
        
        # Fiabilit√© des domaines
        reliable_count = sum(1 for domain in domains if any(rel in domain for rel in reliable_domains))
        if reliable_count > 0:
            score += min(2, reliable_count)
            bonuses.append(f"{reliable_count} source(s) fiable(s) identifi√©e(s)")
        
        return {
            'score': min(10, score),
            'url_count': len(urls),
            'valid_urls': len(valid_urls),
            'domain_diversity': round(domain_diversity, 2),
            'domains': list(domains),
            'issues': issues,
            'bonuses': bonuses
        }
    
    def _evaluate_completeness(self, prompt: str, response: str) -> Dict[str, any]:
        """√âvalue si la r√©ponse r√©pond compl√®tement √† la question"""
        if not response.strip():
            return {'score': 0, 'coverage': 0, 'issues': ['Aucune r√©ponse']}
        
        # Analyse simple bas√©e sur les mots-cl√©s du prompt
        prompt_keywords = set(re.findall(r'\b\w+\b', prompt.lower()))
        prompt_keywords = {word for word in prompt_keywords if len(word) > 3}  # Mots significatifs
        
        response_lower = response.lower()
        covered_keywords = sum(1 for keyword in prompt_keywords if keyword in response_lower)
        coverage = covered_keywords / max(len(prompt_keywords), 1)
        
        # D√©tection des questions dans le prompt
        question_indicators = ['comment', 'pourquoi', 'quoi', 'qui', 'quand', 'o√π', 'combien', '?']
        has_questions = any(indicator in prompt.lower() for indicator in question_indicators)
        
        score = 5  # Score de base
        issues = []
        bonuses = []
        
        # Score bas√© sur la couverture des mots-cl√©s
        if coverage >= 0.7:
            score += 3
            bonuses.append("Couvre bien les √©l√©ments du prompt")
        elif coverage >= 0.5:
            score += 2
        elif coverage >= 0.3:
            score += 1
        else:
            issues.append("Ne couvre pas assez d'√©l√©ments du prompt")
        
        # Longueur proportionnelle √† la complexit√© du prompt
        response_length = len(response.split())
        prompt_length = len(prompt.split())
        
        if prompt_length > 20 and response_length < 50:
            score -= 1
            issues.append("R√©ponse courte pour un prompt complexe")
        elif prompt_length < 10 and response_length > 200:
            score -= 0.5
            issues.append("R√©ponse peut-√™tre trop d√©taill√©e")
        else:
            bonuses.append("Longueur appropri√©e au prompt")
        
        return {
            'score': min(10, max(0, score)),
            'coverage': round(coverage, 2),
            'covered_keywords': covered_keywords,
            'total_keywords': len(prompt_keywords),
            'issues': issues,
            'bonuses': bonuses
        }
    
    def _evaluate_relevance(self, prompt: str, response: str) -> Dict[str, any]:
        """√âvalue la pertinence de la r√©ponse par rapport au prompt"""
        if not response.strip():
            return {'score': 0, 'relevance_indicators': [], 'issues': ['Aucune r√©ponse']}
        
        score = 5  # Score de base
        issues = []
        bonuses = []
        relevance_indicators = []
        
        # D√©tection du type de demande
        prompt_lower = prompt.lower()
        response_lower = response.lower()
        
        # Demande d'explication
        if any(word in prompt_lower for word in ['expliquer', 'comment', 'pourquoi', 'qu\'est-ce']):
            if any(word in response_lower for word in ['parce que', 'car', 'en effet', 'cela signifie']):
                score += 1
                bonuses.append("Fournit des explications")
                relevance_indicators.append("explanatory")
        
        # Demande de liste/√©num√©ration
        if any(word in prompt_lower for word in ['liste', '√©num√®re', 'quels sont', 'exemples']):
            if re.search(r'^\s*[-‚Ä¢*]\s', response, re.MULTILINE) or re.search(r'^\s*\d+\.\s', response, re.MULTILINE):
                score += 1
                bonuses.append("R√©pond avec une structure de liste")
                relevance_indicators.append("list_format")
        
        # Demande d'analyse/comparaison
        if any(word in prompt_lower for word in ['analyser', 'comparer', 'diff√©rence', 'avantages', 'inconv√©nients']):
            if any(word in response_lower for word in ['d\'une part', 'd\'autre part', 'tandis que', 'en revanche', 'contrairement']):
                score += 1
                bonuses.append("Structure comparative appropri√©e")
                relevance_indicators.append("comparative")
        
        # Coh√©rence th√©matique simple (mots-cl√©s communs)
        prompt_words = set(re.findall(r'\b\w{4,}\b', prompt_lower))
        response_words = set(re.findall(r'\b\w{4,}\b', response_lower))
        common_words = prompt_words.intersection(response_words)
        
        if len(common_words) >= 3:
            score += 1
            bonuses.append("Bonne coh√©rence th√©matique")
            relevance_indicators.append("thematic_coherence")
        elif len(common_words) < 1:
            score -= 1
            issues.append("Manque de coh√©rence th√©matique")
        
        # D√©tection de hors-sujet √©vident
        if len(response.split()) > 50 and len(common_words) == 0:
            score -= 2
            issues.append("Possible hors-sujet")
        
        return {
            'score': min(10, max(0, score)),
            'relevance_indicators': relevance_indicators,
            'common_keywords': len(common_words),
            'issues': issues,
            'bonuses': bonuses
        }
    
    def _generate_recommendations(self, readability: Dict, structure: Dict, 
                                sources: Dict, completeness: Dict, relevance: Dict) -> List[str]:
        """G√©n√®re des recommandations d'am√©lioration"""
        recommendations = []
        
        if readability['score'] < 6:
            recommendations.append("üí° Am√©liorer la lisibilit√© : utiliser des phrases plus courtes et un vocabulaire plus simple")
        
        if structure['score'] < 6:
            if 'Manque de paragraphes' in structure.get('issues', []):
                recommendations.append("üìù Structurer en paragraphes pour une meilleure organisation")
            recommendations.append("üî§ Ajouter des titres ou listes pour clarifier la structure")
        
        if sources['score'] < 5:
            recommendations.append("üîó Fournir plus de sources fiables et diversifi√©es")
        elif sources['score'] < 8:
            recommendations.append("üéØ Diversifier les sources (√©viter de se concentrer sur un seul domaine)")
        
        if completeness['score'] < 6:
            recommendations.append("üìã R√©ponse incompl√®te : couvrir plus d'aspects de la question")
        
        if relevance['score'] < 6:
            recommendations.append("üéØ Am√©liorer la pertinence : mieux r√©pondre aux √©l√©ments sp√©cifiques du prompt")
        
        if not recommendations:
            recommendations.append("‚úÖ Excellente r√©ponse ! Maintenir cette qualit√©")
        
        return recommendations
    
    def _avg_sentence_length(self, text: str) -> float:
        """Calcule la longueur moyenne des phrases"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return 0
        total_words = sum(len(sentence.split()) for sentence in sentences)
        return total_words / len(sentences)
    
    def _avg_word_length(self, text: str) -> float:
        """Calcule la longueur moyenne des mots"""
        words = text.split()
        if not words:
            return 0
        return sum(len(word.strip('.,!?;:')) for word in words) / len(words)

# Fonction d'aide pour l'interface Streamlit
def display_evaluation_results(evaluation: EvaluationResult, model_name: str = ""):
    """Affiche les r√©sultats d'√©valuation dans Streamlit"""
    
    # Header avec score global
    score_emoji = "‚≠ê" * int(evaluation.overall_score // 2)
    st.markdown(f"### üìä √âvaluation {model_name}")
    st.markdown(f"**Score Global: {evaluation.overall_score}/10** {score_emoji}")
    
    # Graphique en barres des scores
    import plotly.graph_objects as go
    
    categories = ['Lisibilit√©', 'Structure', 'Sources', 'Compl√©tude', 'Pertinence']
    scores = [
        evaluation.readability_score,
        evaluation.structure_score,
        evaluation.sources_score,
        evaluation.completeness_score,
        evaluation.relevance_score
    ]
    
    fig = go.Figure(data=[
        go.Bar(
            x=categories,
            y=scores,
            marker_color=['#FF6B6B' if s < 5 else '#4ECDC4' if s < 8 else '#45B7D1' for s in scores],
            text=[f'{s}/10' for s in scores],
            textposition='auto'
        )
    ])
    
    fig.update_layout(
        title=f"D√©tail des Scores - {model_name}",
        yaxis_title="Score (/10)",
        yaxis=dict(range=[0, 10]),
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # D√©tails par cat√©gorie
    with st.expander("üìã D√©tails de l'√©valuation"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üìñ Lisibilit√©:**")
            st.write(f"- Niveau: {evaluation.details['readability']['level']}")
            if 'flesch_score' in evaluation.details['readability']['details']:
                st.write(f"- Score Flesch: {evaluation.details['readability']['details']['flesch_score']}")
            
            st.markdown("**üèóÔ∏è Structure:**")
            structure_details = evaluation.details['structure']
            if structure_details.get('bonuses'):
                for bonus in structure_details['bonuses']:
                    st.write(f"‚úÖ {bonus}")
            if structure_details.get('issues'):
                for issue in structure_details['issues']:
                    st.write(f"‚ö†Ô∏è {issue}")
        
        with col2:
            st.markdown("**üîó Sources:**")
            sources_details = evaluation.details['sources']
            st.write(f"- URLs trouv√©es: {sources_details['valid_urls']}")
            st.write(f"- Diversit√© domaines: {sources_details['domain_diversity']}")
            
            st.markdown("**üìä Statistiques:**")
            st.write(f"- Mots: {evaluation.details['word_count']}")
            st.write(f"- Caract√®res: {evaluation.details['char_count']}")
    
    # Recommandations
    if evaluation.recommendations:
        st.markdown("### üí° Recommandations d'am√©lioration:")
        for rec in evaluation.recommendations:
            st.write(f"‚Ä¢ {rec}")

# Fonction pour l'affichage du dashboard comparatif
def display_comparative_dashboard(report: Dict[str, any]):
    """Affiche le dashboard comparatif dans Streamlit"""
    import streamlit as st
    import plotly.express as px
    import plotly.graph_objects as go
    import pandas as pd
    
    st.markdown("## üèÜ Rapport Comparatif des Mod√®les")
    
    # M√©triques principales
    if report['summary']:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            best_model = report['rankings']['overall'][0] if report['rankings']['overall'] else "N/A"
            best_score = report['summary'][best_model]['averages']['overall'] if best_model != "N/A" else 0
            st.metric("ü•á Meilleur Mod√®le", best_model, f"{best_score:.1f}/10")
        
        with col2:
            total_evaluations = sum(data['count'] for data in report['summary'].values())
            st.metric("üìä Total √âvaluations", total_evaluations)
        
        with col3:
            avg_global = sum(data['averages']['overall'] for data in report['summary'].values()) / len(report['summary'])
            st.metric("üìà Score Moyen Global", f"{avg_global:.1f}/10")
    
    # Graphiques comparatifs
    tab1, tab2, tab3 = st.tabs(["üìä Scores Moyens", "üèÖ Classements", "üí° Insights"])
    
    with tab1:
        if report['summary']:
            # Graphique en barres group√©es
            data_for_chart = []
            for model, data in report['summary'].items():
                for criterion, score in data['averages'].items():
                    if criterion != 'overall':  # Exclure le score global pour √©viter la redondance
                        data_for_chart.append({
                            'Mod√®le': model.split(':')[0] if ':' in model else model,
                            'Crit√®re': criterion.title(),
                            'Score': score
                        })
            
            df_chart = pd.DataFrame(data_for_chart)
            
            fig = px.bar(
                df_chart,
                x='Mod√®le',
                y='Score',
                color='Crit√®re',
                title='Scores Moyens par Crit√®re et Mod√®le',
                barmode='group'
            )
            fig.update_layout(yaxis=dict(range=[0, 10]))
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üèÜ Classement Global")
            for i, model in enumerate(report['rankings']['overall'][:5], 1):
                score = report['summary'][model]['averages']['overall']
                medal = ["ü•á", "ü•à", "ü•â", "4Ô∏è‚É£", "5Ô∏è‚É£"][i-1] if i <= 5 else f"{i}."
                model_short = model.split(':')[0] if ':' in model else model
                st.write(f"{medal} {model_short}: {score:.1f}/10")
        
        with col2:
            st.subheader("üéØ Sp√©cialistes par Crit√®re")
            criteria_labels = {
                'readability': 'üìñ Lisibilit√©',
                'structure': 'üèóÔ∏è Structure',
                'sources': 'üîó Sources',
                'completeness': 'üìã Compl√©tude',
                'relevance': 'üéØ Pertinence'
            }
            
            for criterion, label in criteria_labels.items():
                if criterion in report['rankings']:
                    best = report['rankings'][criterion][0]
                    score = report['summary'][best]['averages'][criterion]
                    best_short = best.split(':')[0] if ':' in best else best
                    st.write(f"{label}: **{best_short}** ({score:.1f}/10)")
    
    with tab3:
        st.subheader("üí° Insights Automatiques")
        for insight in report['insights']:
            st.write(f"‚Ä¢ {insight}")
        
        st.subheader("üéØ Recommandations")
        for recommendation in report['recommendations']:
            st.write(f"‚Ä¢ {recommendation}")
        
        # Graphique radar des top 3
        if len(report['summary']) >= 2:
            st.subheader("üì° Comparaison Radar - Top 3")
            
            top_3_models = report['rankings']['overall'][:3]
            
            fig = go.Figure()
            categories = ['Lisibilit√©', 'Structure', 'Sources', 'Compl√©tude', 'Pertinence']
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
            
            for i, model in enumerate(top_3_models):
                if model in report['summary']:
                    values = [
                        report['summary'][model]['averages']['readability'],
                        report['summary'][model]['averages']['structure'],
                        report['summary'][model]['averages']['sources'],
                        report['summary'][model]['averages']['completeness'],
                        report['summary'][model]['averages']['relevance']
                    ]
                    
                    model_short = model.split(':')[0] if ':' in model else model
                    
                    fig.add_trace(go.Scatterpolar(
                        r=values,
                        theta=categories,
                        fill='toself',
                        name=model_short,
                        line_color=colors[i % len(colors)]
                    ))
            
            fig.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 10])),
                showlegend=True,
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)

# Fonction pour sauvegarder les rapports
def save_evaluation_report(report: Dict[str, any], filename: str):
    """Sauvegarde le rapport d'√©valuation en JSON"""
    
    # S'assurer que le dossier existe
    Path(filename).parent.mkdir(parents=True, exist_ok=True)
    
    report_with_metadata = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'version': '1.0',
            'total_models': len(report['summary']),
            'total_evaluations': sum(data['count'] for data in report['summary'].values())
        },
        'report': report
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report_with_metadata, f, ensure_ascii=False, indent=2)